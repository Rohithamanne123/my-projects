{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":29653,"databundleVersionId":2420395,"sourceType":"competition"},{"sourceId":2542390,"sourceType":"datasetVersion","datasetId":1541666}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install git+https://github.com/shijianjian/EfficientNet-PyTorch-3D\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T09:35:40.162153Z","iopub.execute_input":"2024-05-16T09:35:40.162819Z","iopub.status.idle":"2024-05-16T09:36:00.253849Z","shell.execute_reply.started":"2024-05-16T09:35:40.162778Z","shell.execute_reply":"2024-05-16T09:36:00.252450Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"execution":{"iopub.status.busy":"2024-05-16T09:36:00.256156Z","iopub.execute_input":"2024-05-16T09:36:00.256517Z","iopub.status.idle":"2024-05-16T09:36:17.812979Z","shell.execute_reply.started":"2024-05-16T09:36:00.256485Z","shell.execute_reply":"2024-05-16T09:36:17.811738Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet_pytorch) (2.1.2+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=70ea51f630928d7e563794d618b92b788d7efcd3b2234f02b0a2c22a12fa84ee\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: efficientnet_pytorch\nSuccessfully installed efficientnet_pytorch-0.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"pip show pydicom","metadata":{"execution":{"iopub.status.busy":"2024-05-16T09:36:17.814826Z","iopub.execute_input":"2024-05-16T09:36:17.815210Z","iopub.status.idle":"2024-05-16T09:36:31.960998Z","shell.execute_reply.started":"2024-05-16T09:36:17.815178Z","shell.execute_reply":"2024-05-16T09:36:31.959604Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Name: pydicom\nVersion: 2.4.4\nSummary: A pure Python package for reading and writing DICOM data\nHome-page: \nAuthor: \nAuthor-email: Darcy Mason and contributors <darcymason@gmail.com>\nLicense: \nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: \nRequired-by: \nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Specify your dataset directory\ndataset_dir = \"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification\"\n# Get a list of all file names in the dataset directory\nfile_names = [f for f in os.listdir(dataset_dir) if os.path.isfile(os.path.join(dataset_dir, f))]\n\nprint(file_names)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T09:36:31.964161Z","iopub.execute_input":"2024-05-16T09:36:31.964665Z","iopub.status.idle":"2024-05-16T09:36:31.974806Z","shell.execute_reply.started":"2024-05-16T09:36:31.964613Z","shell.execute_reply":"2024-05-16T09:36:31.973591Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['sample_submission.csv', 'train_labels.csv']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Let's start with the first step: Data Preprocessing. Here's how you can load DICOM images from each patient directory and preprocess them using Python with the PyDICOM and OpenCV libraries:","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pydicom\nimport glob\n\ndef load_dicom_images(patient_dir):\n    \"\"\"\n    Load DICOM images from a patient directory.\n    \n    Parameters:\n    - patient_dir: Path to the directory containing DICOM images.\n    \n    Returns:\n    - images: List of loaded DICOM images.\n    \"\"\"\n    images = []\n    for filepath in glob.glob(os.path.join(patient_dir, '**', '*.dcm'), recursive=True):\n        try:\n            ds = pydicom.dcmread(filepath)\n            img = ds.pixel_array\n            images.append(img)\n        except Exception as e:\n            pass\n    return images\n\ndef preprocess_images(images):\n    \"\"\"\n    Preprocess a list of images.\n​\n    Parameters:\n    - images: List of input images.\n​\n    Returns:\n    - preprocessed_images: List of preprocessed images.\n    \"\"\"\n    preprocessed_images = []\n    for img in images:\n        # Resize image to desired dimensions (e.g., 256x256)\n        img = cv2.resize(img, (256, 256))\n        # Normalize pixel values (if needed)\n        img = img / 255.0\n        preprocessed_images.append(img)\n    return preprocessed_images\n\n## Example usage:\ndataset_dir = \"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00000/FLAIR/\"\nprint(\"Files in dataset directory:\", os.listdir(dataset_dir)[:5])  # Corrected line\n\n# Debug print: Print first 5 characters of DICOM file paths\ncount = 0\nfor root, dirs, files in os.walk(dataset_dir):\n    for file in files:\n        if file.endswith(\".dcm\"):\n            print(os.path.join(root, file))\n            count += 1\n            if count >= 5:\n                break\n    if count >= 5:\n        break\n        \nimages = load_dicom_images(dataset_dir)\nprint(\"Number of DICOM images loaded:\", len(images))\npreprocessed_images = preprocess_images(images)\nprint(\"Number of preprocessed images:\", len(preprocessed_images))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T09:37:28.570559Z","iopub.execute_input":"2024-05-16T09:37:28.570936Z","iopub.status.idle":"2024-05-16T09:37:34.757737Z","shell.execute_reply.started":"2024-05-16T09:37:28.570908Z","shell.execute_reply":"2024-05-16T09:37:34.756520Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Files in dataset directory: ['Image-273.dcm', 'Image-245.dcm', 'Image-365.dcm', 'Image-130.dcm', 'Image-98.dcm']\n/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00000/FLAIR/Image-273.dcm\n/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00000/FLAIR/Image-245.dcm\n/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00000/FLAIR/Image-365.dcm\n/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00000/FLAIR/Image-130.dcm\n/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00000/FLAIR/Image-98.dcm\nNumber of DICOM images loaded: 400\nNumber of preprocessed images: 400\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nnum_files = sum([len(files) for r, d, files in os.walk(\"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/\")])\nprint(f\"Total number of DICOM files: {num_files}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T09:44:15.518839Z","iopub.execute_input":"2024-05-16T09:44:15.520451Z","iopub.status.idle":"2024-05-16T09:46:32.812306Z","shell.execute_reply.started":"2024-05-16T09:44:15.520401Z","shell.execute_reply":"2024-05-16T09:46:32.811158Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Total number of DICOM files: 400116\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\ntotal_size = 0\nnum_files = 0\nfor dirpath, dirnames, filenames in os.walk(\"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/\"):\n    for f in filenames:\n        fp = os.path.join(dirpath, f)\n        total_size += os.path.getsize(fp)\n        num_files += 1\navg_size_mb = (total_size / num_files) / (1024 * 1024)  # Convert bytes to MB\nprint(f\"Average file size: {avg_size_mb:.2f} MB\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T09:49:43.073448Z","iopub.execute_input":"2024-05-16T09:49:43.074106Z","iopub.status.idle":"2024-05-16T09:53:04.980482Z","shell.execute_reply.started":"2024-05-16T09:49:43.074073Z","shell.execute_reply":"2024-05-16T09:53:04.979219Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Average file size: 0.33 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"import psutil\nimport os\nimport subprocess\n\nprint(\"CPU Info:\", os.cpu_count())\nprint(\"RAM Info:\", psutil.virtual_memory())\ngpu_info = subprocess.check_output(['nvidia-smi']).decode('utf-8')\nprint(\"GPU Info:\", gpu_info)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T09:57:42.018432Z","iopub.execute_input":"2024-05-16T09:57:42.018815Z","iopub.status.idle":"2024-05-16T09:57:42.076251Z","shell.execute_reply.started":"2024-05-16T09:57:42.018786Z","shell.execute_reply":"2024-05-16T09:57:42.075337Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"CPU Info: 4\nRAM Info: svmem(total=33669926912, available=32513171456, percent=3.4, used=678666240, free=26040758272, active=1685721088, inactive=5176193024, buffers=1380655104, cached=5569847296, shared=999424, slab=549867520)\nGPU Info: Thu May 16 09:57:42 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0              26W / 250W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nimport pydicom\nimport cv2\n\nstart_time = time.time()\nds = pydicom.dcmread('/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00000/FLAIR/Image-1.dcm')  # Provide a sample DICOM file path\nimg = ds.pixel_array\nimg = cv2.resize(img, (256, 256))\nimg = img / 255.0\nend_time = time.time()\nprint(f\"Time taken to preprocess one image: {end_time - start_time:.2f} seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T09:58:07.065073Z","iopub.execute_input":"2024-05-16T09:58:07.065416Z","iopub.status.idle":"2024-05-16T09:58:07.110671Z","shell.execute_reply.started":"2024-05-16T09:58:07.065388Z","shell.execute_reply":"2024-05-16T09:58:07.109733Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Time taken to preprocess one image: 0.04 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimg = np.expand_dims(img, axis=-1)  # Use the preprocessed image from the previous step\nbatch_img = np.expand_dims(img, axis=0)  # Create a batch with one image\naugmenter = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='nearest'\n)\nstart_time = time.time()\naugmented_img = next(augmenter.flow(batch_img, batch_size=1))\nend_time = time.time()\nprint(f\"Time taken to augment one image: {end_time - start_time:.2f} seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T09:58:10.081247Z","iopub.execute_input":"2024-05-16T09:58:10.081595Z","iopub.status.idle":"2024-05-16T09:58:21.617958Z","shell.execute_reply.started":"2024-05-16T09:58:10.081567Z","shell.execute_reply":"2024-05-16T09:58:21.616938Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-16 09:58:11.744612: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-16 09:58:11.744707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-16 09:58:11.873532: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Time taken to augment one image: 0.03 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pydicom\nimport cv2\nimport glob\nimport multiprocessing\n\ndef load_dicom_images(patient_dir):\n    \"\"\"\n    Load DICOM images from a patient directory.\n    \n    Parameters:\n    - patient_dir: Path to the directory containing DICOM images.\n    \n    Returns:\n    - images: List of loaded DICOM images.\n    \"\"\"\n    images = []\n    for filepath in glob.glob(os.path.join(patient_dir, '**', '*.dcm'), recursive=True):\n        try:\n            ds = pydicom.dcmread(filepath)\n            img = ds.pixel_array\n            images.append(img)\n        except Exception as e:\n            pass\n    return images\n\ndef preprocess_images(images):\n    \"\"\"\n    Preprocess a list of images.\n    \n    Parameters:\n    - images: List of input images.\n    \n    Returns:\n    - preprocessed_images: List of preprocessed images.\n    \"\"\"\n    preprocessed_images = []\n    for img in images:\n        # Resize image to desired dimensions (e.g., 256x256)\n        img = cv2.resize(img, (256, 256))\n        # Normalize pixel values\n        img = img / 255.0\n        preprocessed_images.append(img)\n    return preprocessed_images\n\n# Define the dataset directory\ndataset_dir = \"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/\"\n\n# Load DICOM images from 100 patient directories using multiprocessing\npatient_dirs = [os.path.join(dataset_dir, dir) for dir in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, dir))][:100]\n\npool = multiprocessing.Pool(processes=4)  # Adjust the number of processes as needed\nimages_list = pool.map(load_dicom_images, patient_dirs)\npool.close()\npool.join()\n\n# Flatten the list of lists into a single list of images\nimages = [img for sublist in images_list for img in sublist]\n\n# Preprocess the images\npreprocessed_images = preprocess_images(images)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T10:06:17.686816Z","iopub.execute_input":"2024-05-16T10:06:17.687503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The second step is Data Augmentation. Data augmentation is crucial for training robust machine learning models, especially when dealing with limited datasets. Here's how you can augment your preprocessed images using the ImageDataGenerator class from Keras","metadata":{}},{"cell_type":"markdown","source":"import os\nimport cv2\nimport numpy as np\nimport pydicom\nimport glob\nimport pandas as pd\nfrom multiprocessing import Pool\nfrom functools import partial\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\ndef load_labels(file_path):\n    \"\"\"\n    Load MGMT values from the train_labels.csv file into a dictionary.\n    \n    Parameters:\n    - file_path: Path to the train_labels.csv file.\n    \n    Returns:\n    - labels_dict: Dictionary mapping patient IDs to MGMT values.\n    \"\"\"\n    labels_df = pd.read_csv(file_path)\n    labels_dict = dict(zip(labels_df['BraTS21ID'], labels_df['MGMT_value']))\n    return labels_dict\n\ndef load_dicom(filepath):\n    \"\"\"\n    Load and preprocess a single DICOM image.\n    \n    Parameters:\n    - filepath: Path to the DICOM file.\n    \n    Returns:\n    - img: Preprocessed image.\n    \"\"\"\n    try:\n        ds = pydicom.dcmread(filepath)\n        img = ds.pixel_array\n        img = cv2.resize(img, (256, 256))\n        img = img / 255.0\n        return img\n    except Exception as e:\n        return None\n\ndef load_dicom_batch(filepaths):\n    \"\"\"\n    Load and preprocess a batch of DICOM images in parallel.\n    \n    Parameters:\n    - filepaths: List of file paths to DICOM files.\n    \n    Returns:\n    - batch_images: Batch of preprocessed images.\n    \"\"\"\n    with Pool() as pool:\n        batch_images = pool.map(load_dicom, filepaths)\n    return np.array(batch_images)\n\ndef load_dicom_images_generator(dataset_dir, labels_dict, batch_size=256):\n    \"\"\"\n    Generator function to load DICOM images from patient directories in batches.\n    \n    Parameters:\n    - dataset_dir: Path to the directory containing patient directories.\n    - labels_dict: Dictionary mapping patient IDs to MGMT values.\n    - batch_size: Number of images to load per batch.\n    \n    Yields:\n    - batch_images: Batch of preprocessed images.\n    - batch_labels: Batch of MGMT labels.\n    \"\"\"\n    for patient_dir in os.listdir(dataset_dir):\n        patient_path = os.path.join(dataset_dir, patient_dir)\n        if os.path.isdir(patient_path):\n            filepaths = glob.glob(os.path.join(patient_path, '**', '*.dcm'), recursive=True)\n            filepaths = filepaths[:batch_size]  # Select only first batch_size files\n            batch_images = load_dicom_batch(filepaths)\n            patient_id = os.path.basename(patient_path)\n            mgmt_value = labels_dict.get(patient_id, -1)  # Default value of -1 if not found\n            yield batch_images, mgmt_value\n\n# Data augmentation using Keras ImageDataGenerator\ndata_augmenter = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='nearest'\n)\n\n# Example usage:\ndataset_dir = \"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/\"\nlabels_file = \"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv\"\n\n# Load MGMT labels\nlabels_dict = load_labels(labels_file)\n\n# Load DICOM images and labels using the generator\nimage_label_generator = load_dicom_images_generator(dataset_dir, labels_dict)\n\n# Process images and labels in batches with data augmentation\nfor batch_index, (batch_images, batch_labels) in enumerate(image_label_generator):\n    # Reshape batch_images to have rank 4\n    batch_images = np.expand_dims(batch_images, axis=-1)  # Add channel dimension\n    batch_images = np.repeat(batch_images, 3, axis=-1)  # Repeat grayscale image to 3 channels\n    augmented_images = next(data_augmenter.flow(batch_images, batch_size=256, shuffle=False))\n    print(f\"Batch {batch_index + 1}: Augmented images - {augmented_images.shape}, Labels - {batch_labels}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T16:10:53.084103Z","iopub.execute_input":"2024-05-15T16:10:53.084597Z","iopub.status.idle":"2024-05-15T16:23:19.842796Z","shell.execute_reply.started":"2024-05-15T16:10:53.084566Z","shell.execute_reply":"2024-05-15T16:23:19.841406Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Setup and Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pydicom\nimport pandas as pd\nimport glob\nimport logging\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Ensure you run this segment first\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T08:06:49.803950Z","iopub.execute_input":"2024-05-16T08:06:49.804358Z","iopub.status.idle":"2024-05-16T08:06:49.810136Z","shell.execute_reply.started":"2024-05-16T08:06:49.804324Z","shell.execute_reply":"2024-05-16T08:06:49.809097Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# 2. Logging Configuration","metadata":{}},{"cell_type":"code","source":"# Set up logging\nlogging.basicConfig(filename='progress.log', level=logging.INFO)\n\ndef log_progress(batch_index, batch_images, batch_labels):\n    logging.info(f\"Batch {batch_index + 1}: Processed {len(batch_images)} images with labels {batch_labels}\")\n    if batch_index % 10 == 0:  # Print progress every 10 batches\n        print(f\"Processed {batch_index + 1} batches.\")\n\n# Ensure you run this segment second\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T08:06:59.890341Z","iopub.execute_input":"2024-05-16T08:06:59.890741Z","iopub.status.idle":"2024-05-16T08:06:59.897194Z","shell.execute_reply.started":"2024-05-16T08:06:59.890710Z","shell.execute_reply":"2024-05-16T08:06:59.895947Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# 3. Generator Function Definition","metadata":{}},{"cell_type":"code","source":"def load_dicom_images_generator(dataset_dir, batch_size=32):\n    \"\"\"\n    Generator function to load DICOM images from patient directories in batches.\n    \n    Parameters:\n    - dataset_dir: Path to the directory containing patient directories.\n    - batch_size: Number of images to load per batch.\n    \n    Yields:\n    - batch_images: Batch of preprocessed images.\n    - batch_labels: Batch of corresponding labels.\n    \"\"\"\n    train_labels = pd.read_csv(\"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv\")\n    image_paths = []\n    labels = []\n\n    for patient_dir in os.listdir(dataset_dir):\n        patient_path = os.path.join(dataset_dir, patient_dir)\n        if os.path.isdir(patient_path):\n            for filepath in glob.glob(os.path.join(patient_path, '*.dcm')):\n                image_id = os.path.basename(filepath).split('-')[1].split('.')[0]\n                label_data = train_labels[train_labels['BraTS21ID'] == int(image_id)]['MGMT_value']\n                if not label_data.empty:\n                    label = label_data.values[0]\n                    labels.append(label)\n                    image_paths.append(filepath)\n    \n    num_images = len(image_paths)\n    indices = np.random.permutation(num_images)\n    \n    while True:\n        for i in range(0, num_images, batch_size):\n            batch_indices = indices[i:i+batch_size]\n            batch_images = []\n            batch_labels = []\n            for idx in batch_indices:\n                filepath = image_paths[idx]\n                try:\n                    ds = pydicom.dcmread(filepath)\n                    img = ds.pixel_array\n                    img = cv2.resize(img, (256, 256))\n                    img = img / 255.0\n                    batch_images.append(img)\n                    batch_labels.append(labels[idx])\n                except Exception as e:\n                    logging.error(f\"Error processing {filepath}: {e}\")\n            \n            yield np.array(batch_images), np.array(batch_labels)\n\n# Ensure you run this segment third\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T08:07:12.332378Z","iopub.execute_input":"2024-05-16T08:07:12.332832Z","iopub.status.idle":"2024-05-16T08:07:12.345577Z","shell.execute_reply.started":"2024-05-16T08:07:12.332800Z","shell.execute_reply":"2024-05-16T08:07:12.344407Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Augmentation Setup","metadata":{}},{"cell_type":"code","source":"# Data augmentation\ndata_augmenter = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='nearest'\n)\n\n# Ensure you run this segment fourth\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T08:07:31.951727Z","iopub.execute_input":"2024-05-16T08:07:31.952145Z","iopub.status.idle":"2024-05-16T08:07:31.957719Z","shell.execute_reply.started":"2024-05-16T08:07:31.952098Z","shell.execute_reply":"2024-05-16T08:07:31.956578Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# 5. Main Processing Loop with Checkpoints","metadata":{}},{"cell_type":"code","source":"# Example usage\ndataset_dir = \"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/\"\nbatch_size = 32\n\n# Load DICOM images and labels using the generator\nimage_label_generator = load_dicom_images_generator(dataset_dir, batch_size=batch_size)\n\n# Ensure you run this segment fifth\n\nimport time\n\n# Process images and labels in batches with data augmentation\nfor batch_index, (batch_images, batch_labels) in enumerate(image_label_generator):\n    start_time = time.time()\n    \n    # Perform data augmentation\n    augmented_generator = data_augmenter.flow(np.expand_dims(batch_images, axis=-1), batch_labels, batch_size=batch_size, shuffle=False)\n    \n    # Iterate over augmented batches\n    for augmented_images, augmented_labels in augmented_generator:\n        log_progress(batch_index, augmented_images, augmented_labels)\n        break  # Stop after processing the first augmented batch in the generator\n\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    \n    # Print elapsed time for processing the batch\n    print(f\"Batch {batch_index + 1} processed in {elapsed_time:.2f} seconds.\")\n    \n    # Checkpoint: Save progress every 10 batches\n    if (batch_index + 1) % 10 == 0:\n        print(f\"Processed {batch_index + 1} batches.\")\n\n    # For testing purposes, limit to a few batches to observe initial output\n    if batch_index >= 9:  # Change this number as needed for testing\n        break\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T08:09:37.631530Z","iopub.execute_input":"2024-05-16T08:09:37.631907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nnum_files = sum([len(files) for r, d, files in os.walk(\"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/\")])\nprint(f\"Total number of DICOM files: {num_files}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}